"""
Automated Codebase Summarization & LLM-to-LLM Transfer Engine

This module implements the 11-step checklist for comprehensive codebase analysis,
summarization, and secure transfer to target LLM systems.
"""

import asyncio
import json
import logging
import hashlib
import zipfile
import os
import subprocess
import tempfile
import fnmatch
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
import base64

import logging

from ..config import config

logger = logging.getLogger(__name__)


@dataclass
class ScopeConfig:
    """Step 1: Scoping & Guardrails configuration"""
    repos: List[str]
    branches: List[str]
    submodules: bool = True
    include_generated: bool = False
    include_data_files: bool = True
    exclude_patterns: List[str] = None
    redflags: List[str] = None
    target_llm_profile: Dict[str, Any] = None

    def __post_init__(self):
        if self.exclude_patterns is None:
            self.exclude_patterns = [
                "*.log", "*.tmp", "node_modules/", "__pycache__/", 
                ".git/", "*.pyc", "*.pyo", ".DS_Store"
            ]
        if self.redflags is None:
            self.redflags = [
                "password", "secret", "api_key", "token", "private_key",
                "ssn", "credit_card", "license", "proprietary"
            ]
        if self.target_llm_profile is None:
            self.target_llm_profile = {
                "context_window": 128000,
                "input_limits": 100000,
                "accepted_formats": ["json", "markdown", "yaml"],
                "rate_limits": {"requests_per_minute": 60}
            }


@dataclass
class FileMetadata:
    """Metadata for individual source files"""
    path: str
    lines_of_code: int
    language: str
    last_modified: str
    author: str
    complexity_score: float
    size_bytes: int
    encoding: str
    purpose: str = ""
    dependencies: List[str] = None
    issues: List[str] = None
    todos: List[str] = None

    def __post_init__(self):
        if self.dependencies is None:
            self.dependencies = []
        if self.issues is None:
            self.issues = []
        if self.todos is None:
            self.todos = []


@dataclass
class ComponentMetadata:
    """Metadata for logical components (packages, modules, classes)"""
    name: str
    type: str  # package, module, class, function
    purpose: str
    inputs: List[str]
    outputs: List[str]
    dependencies: List[str]
    known_issues: List[str]
    todos: List[str]
    api_signature: str = ""
    complexity_score: float = 0.0


@dataclass
class SummaryBundle:
    """Complete summarization bundle for transfer"""
    metadata: Dict[str, Any]
    global_overview: str
    hierarchical_summaries: Dict[str, Any]
    chunk_manifest: List[Dict[str, Any]]
    quality_metrics: Dict[str, Any]
    checksum: str
    created_at: str
    version: str


class CodebaseSummarizationEngine:
    """Main engine implementing the 11-step summarization process"""

    def __init__(self, scope_config: ScopeConfig, workspace_path: str):
        self.scope = scope_config
        self.workspace_path = Path(workspace_path)
        self.output_dir = self.workspace_path / "analysis_artifacts" / "summarization"
        self.output_dir.mkdir(parents=True, exist_ok=True)
          # Initialize step trackers
        self.file_metadata: Dict[str, FileMetadata] = {}
        self.component_metadata: Dict[str, ComponentMetadata] = {}
        self.dependency_graph: Dict[str, List[str]] = {}
        self.test_coverage: Dict[str, float] = {}
        self.performance_metrics: Dict[str, Any] = {}
        self.quality_gates_passed = False
        
        # Load gitignore patterns for intelligent file filtering
        self.gitignore_patterns = self._load_gitignore_patterns()
        
    def _load_gitignore_patterns(self) -> List[str]:
        """Load and parse .gitignore file patterns"""
        gitignore_file = self.workspace_path / ".gitignore"
        patterns = []
        
        if gitignore_file.exists():
            try:
                with open(gitignore_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        # Skip empty lines and comments
                        if line and not line.startswith('#'):
                            # Remove leading slash and convert to glob pattern
                            if line.startswith('/'):
                                line = line[1:]
                            patterns.append(line)
                logger.info(f"Loaded {len(patterns)} patterns from .gitignore")
            except Exception as e:
                logger.warning(f"Could not read .gitignore: {e}")
        else:
            logger.info("No .gitignore file found, using default exclusion patterns")
              # Add some essential patterns if not present
        essential_patterns = [
            "__pycache__", "*.pyc", "*.pyo", ".git", ".pytest_cache", 
            "node_modules", ".mypy_cache", ".ruff_cache", "nca_env",
            "venv", ".venv", "env", ".env", "virtualenv", ".virtualenv",
            "site-packages", "*.egg-info", "build", "dist", ".tox"
        ]
        for pattern in essential_patterns:
            if pattern not in patterns:
                patterns.append(pattern)
                
        return patterns
        
    async def execute_full_pipeline(self) -> SummaryBundle:
        """Execute all 11 steps of the summarization pipeline"""
        logger.info("Starting automated codebase summarization pipeline")
        
        try:
            # Step 1: Already configured in __init__
            logger.info("Step 1: Scoping & Guardrails - Configured")
            
            # Step 2: Environment Prep
            await self._prepare_environment()
            
            # Step 3: Static Harvest
            await self._static_analysis()
            
            # Step 4: Dynamic Insights
            await self._dynamic_analysis()
            
            # Step 5: Metadata & Context Packaging
            await self._package_metadata()
            
            # Step 6: Chunking & Summarization
            chunks = await self._chunk_and_summarize()
            
            # Step 7: Quality Gates
            await self._quality_validation()
            
            # Step 8: Secure Transfer Prep
            bundle = await self._prepare_secure_bundle(chunks)
            
            # Step 9: Post-Transfer Validation (preparation)
            await self._prepare_validation_suite()
            
            # Step 10: Automation Integration
            await self._setup_automation()
            
            # Step 11: Documentation & Handoff
            await self._generate_documentation()
            
            logger.info("Summarization pipeline completed successfully")
            return bundle
            
        except Exception as e:
            logger.error(f"Pipeline failed: {str(e)}")
            raise

    async def _prepare_environment(self):
        """Step 2: Environment Prep - Isolated container with build toolchain"""
        logger.info("Step 2: Preparing isolated analysis environment")
        
        # Create isolated analysis directory
        analysis_env = self.output_dir / "environment"
        analysis_env.mkdir(exist_ok=True)
        
        # Capture dependency snapshots
        await self._capture_dependency_snapshot()
          # Verify build toolchain
        await self._verify_build_tools()
        
    async def _capture_dependency_snapshot(self):
        """Capture comprehensive dependency information"""
        logger.info("Capturing dependency snapshots")
        
        dependency_file = self.output_dir / "dependencies.json"
        dependencies = {}
        
        # Python dependencies (Poetry/pip)
        if (self.workspace_path / "pyproject.toml").exists():
            try:
                logger.info("Running poetry show --tree with timeout...")
                result = subprocess.run(
                    ["poetry", "show", "--tree"], 
                    capture_output=True, text=True, cwd=self.workspace_path,
                    timeout=30  # Add 30-second timeout
                )
                dependencies["poetry"] = result.stdout
                logger.info("Poetry command completed successfully")
            except subprocess.TimeoutExpired:
                logger.warning("Poetry show command timed out after 30 seconds")
                dependencies["poetry"] = "Command timed out"
            except subprocess.CalledProcessError as e:
                logger.warning(f"Poetry not available (exit code {e.returncode}), trying pip")
                try:
                    result = subprocess.run(
                        ["pip", "list"], 
                        capture_output=True, text=True,
                        timeout=15  # Add timeout to pip as well
                    )
                    dependencies["pip"] = result.stdout
                except subprocess.TimeoutExpired:
                    logger.warning("Pip list command timed out")
                    dependencies["pip"] = "Command timed out"
                except subprocess.CalledProcessError:
                    logger.warning("Could not capture Python dependencies")
            except FileNotFoundError:
                logger.warning("Poetry command not found, trying pip")
                try:
                    result = subprocess.run(
                        ["pip", "list"], 
                        capture_output=True, text=True,
                        timeout=15
                    )
                    dependencies["pip"] = result.stdout
                except (subprocess.CalledProcessError, subprocess.TimeoutExpired, FileNotFoundError):
                    logger.warning("Could not capture Python dependencies")
        
        # Node.js dependencies
        if (self.workspace_path / "package.json").exists():
            try:
                result = subprocess.run(
                    ["npm", "ls", "--all"], 
                    capture_output=True, text=True, cwd=self.workspace_path,
                    timeout=30  # Add timeout
                )
                dependencies["npm"] = result.stdout
            except subprocess.TimeoutExpired:
                logger.warning("npm ls command timed out")
                dependencies["npm"] = "Command timed out"
            except subprocess.CalledProcessError:
                logger.warning("Could not capture Node.js dependencies")
            except FileNotFoundError:
                logger.warning("npm command not found")
          # Save dependencies
        try:
            with open(dependency_file, 'w') as f:
                json.dump(dependencies, f, indent=2)
            logger.info(f"Dependencies saved to {dependency_file}")
        except Exception as e:
            logger.error(f"Failed to save dependencies: {e}")
            
    async def _verify_build_tools(self):
        """Verify required build tools are available"""
        logger.info("Verifying build toolchain")
        
        tools = {
            "python": ["python", "--version"],
            "poetry": ["poetry", "--version"],
            "git": ["git", "--version"],
            "node": ["node", "--version"],
            "npm": ["npm", "--version"]
        }
        
        available_tools = {}
        for tool, cmd in tools.items():
            try:
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)
                available_tools[tool] = result.stdout.strip()
                logger.debug(f"Tool {tool}: available")
            except subprocess.TimeoutExpired:
                logger.warning(f"Tool {tool}: command timed out")
                available_tools[tool] = "Command timed out"
            except (subprocess.CalledProcessError, FileNotFoundError):
                logger.debug(f"Tool {tool}: not available")
                available_tools[tool] = "Not available"
            except Exception as e:
                logger.warning(f"Tool {tool}: error checking - {e}")
                available_tools[tool] = f"Error: {e}"
        
        # Save tool inventory
        try:
            with open(self.output_dir / "build_tools.json", 'w') as f:
                json.dump(available_tools, f, indent=2)
            logger.info(f"Build tools inventory saved")
        except Exception as e:
            logger.error(f"Failed to save build tools inventory: {e}")

    async def _static_analysis(self):
        """Step 3: Static Harvest - Comprehensive static code analysis"""
        logger.info("Step 3: Performing static code analysis")
        
        # File enumeration with metadata
        await self._enumerate_files()
        
        # Extract API signatures
        await self._extract_api_signatures()
        
        # Collect configuration
        await self._collect_configuration()
          # Analyze test structure
        await self._analyze_tests()
        
    async def _enumerate_files(self):
        """Enumerate all files with comprehensive metadata"""
        logger.info("Enumerating files and extracting metadata")
        for file_path in self.workspace_path.rglob("*"):
            if file_path.is_file() and not self._should_exclude_file(file_path):
                metadata = await self._extract_file_metadata(file_path)
                self.file_metadata[str(file_path.relative_to(self.workspace_path))] = metadata
                
    def _should_exclude_file(self, file_path: Path) -> bool:
        """Check if file should be excluded based on .gitignore patterns and scope settings"""
        try:
            relative_path = file_path.relative_to(self.workspace_path)
            relative_path_str = str(relative_path)
            relative_path_posix = relative_path.as_posix()  # Use forward slashes
            
            # Check against .gitignore patterns
            for pattern in self.gitignore_patterns:
                # Handle directory patterns (ending with /)
                if pattern.endswith('/'):
                    pattern_for_dirs = pattern.rstrip('/')
                    # Check if any parent directory matches
                    for parent in relative_path.parents:
                        if fnmatch.fnmatch(str(parent), pattern_for_dirs) or \
                           fnmatch.fnmatch(parent.as_posix(), pattern_for_dirs):
                            return True
                else:
                    # Check file and directory patterns
                    if fnmatch.fnmatch(relative_path_str, pattern) or \
                       fnmatch.fnmatch(relative_path_posix, pattern) or \
                       fnmatch.fnmatch(file_path.name, pattern):
                        return True
                    
                    # Check if pattern matches any parent directory
                    for parent in relative_path.parents:
                        if fnmatch.fnmatch(str(parent), pattern) or \
                           fnmatch.fnmatch(parent.as_posix(), pattern):
                            return True
            
            # Check against scope exclude patterns (legacy support)
            for pattern in self.scope.exclude_patterns:
                if fnmatch.fnmatch(relative_path_str, pattern) or pattern in relative_path_str:
                    return True
              # Additional size-based exclusion for very large files
            try:
                if file_path.is_file() and file_path.stat().st_size > 50 * 1024 * 1024:  # 50MB
                    logger.debug(f"Excluding large file: {relative_path_str}")
                    return True
            except OSError:
                pass  # File might not be accessible
            
            # Enhanced filtering for better performance
            # Only process source code and documentation files
            source_extensions = {
                '.py', '.js', '.ts', '.jsx', '.tsx', '.java', '.cpp', '.c', '.h', '.hpp', 
                '.cs', '.go', '.rs', '.php', '.rb', '.swift', '.kt', '.scala', '.r',
                '.sql', '.sh', '.bash', '.ps1', '.yaml', '.yml', '.json', '.toml',
                '.md', '.rst', '.txt', '.cfg', '.ini', '.conf', '.xml', '.html', '.css'
            }
            
            file_ext = file_path.suffix.lower()
            if file_ext and file_ext not in source_extensions:
                return True  # Exclude non-source files
            
            # Exclude files in virtual environments or build directories
            path_parts = relative_path.parts
            exclude_dirs = {
                'nca_env', 'venv', '.venv', 'env', 'virtualenv', '.virtualenv',
                'site-packages', 'node_modules', '__pycache__', '.git', 
                'build', 'dist', '.tox', '.mypy_cache', '.pytest_cache',
                'coverage', '.coverage', 'htmlcov'
            }
            
            for part in path_parts:
                if part.lower() in exclude_dirs:
                    return True
                
            return False
            
        except ValueError:
            # File is not relative to workspace (shouldn't happen)
            return True
        except Exception as e:
            logger.warning(f"Error checking exclusion for {file_path}: {e}")
            return True  # Exclude on error to be safe
        
    async def _extract_file_metadata(self, file_path: Path) -> FileMetadata:
        """Extract comprehensive metadata for a single file"""
        try:
            stat = file_path.stat()
            
            # Detect language
            language = self._detect_language(file_path)
            
            # Count lines of code
            loc = 0
            encoding = 'utf-8'
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    loc = len([l for l in lines if l.strip() and not l.strip().startswith('#')])
            except UnicodeDecodeError:
                try:
                    with open(file_path, 'r', encoding='latin-1') as f:
                        lines = f.readlines()
                        loc = len([l for l in lines if l.strip()])
                        encoding = 'latin-1'
                except:
                    loc = 0
                    encoding = 'binary'
            except:
                loc = 0
            
            # Get git info
            author = await self._get_file_author(file_path)
            last_modified = datetime.fromtimestamp(stat.st_mtime, tz=timezone.utc).isoformat()
            
            # Calculate complexity (simplified)
            complexity = await self._calculate_complexity(file_path, language)
            
            return FileMetadata(
                path=str(file_path.relative_to(self.workspace_path)),
                lines_of_code=loc,
                language=language,
                last_modified=last_modified,
                author=author,
                complexity_score=complexity,
                size_bytes=stat.st_size,
                encoding=encoding
            )
            
        except Exception as e:
            logger.warning(f"Error extracting metadata for {file_path}: {e}")
            return FileMetadata(
                path=str(file_path.relative_to(self.workspace_path)),
                lines_of_code=0,
                language="unknown",
                last_modified="",
                author="unknown",
                complexity_score=0.0,
                size_bytes=0,
                encoding="unknown"
            )
    
    def _detect_language(self, file_path: Path) -> str:
        """Detect programming language from file extension"""
        ext_map = {
            '.py': 'python',
            '.js': 'javascript',
            '.ts': 'typescript',
            '.java': 'java',
            '.cpp': 'cpp',
            '.c': 'c',
            '.h': 'c_header',
            '.hpp': 'cpp_header',
            '.cs': 'csharp',
            '.go': 'go',
            '.rs': 'rust',
            '.rb': 'ruby',
            '.php': 'php',
            '.swift': 'swift',
            '.kt': 'kotlin',
            '.scala': 'scala',
            '.r': 'r',
            '.sql': 'sql',
            '.sh': 'shell',
            '.bash': 'bash',
            '.ps1': 'powershell',
            '.yaml': 'yaml',
            '.yml': 'yaml',
            '.json': 'json',
            '.xml': 'xml',
            '.html': 'html',
            '.css': 'css',
            '.scss': 'scss',
            '.less': 'less',
            '.md': 'markdown',
            '.txt': 'text',
            '.toml': 'toml',
            '.ini': 'ini',
            '.cfg': 'config'
        }
        
        return ext_map.get(file_path.suffix.lower(), 'unknown')
    
    async def _get_file_author(self, file_path: Path) -> str:
        """Get primary author of file from git history"""
        try:
            result = subprocess.run(
                ["git", "log", "--format=%an", "--", str(file_path)],
                capture_output=True, text=True, cwd=self.workspace_path
            )
            authors = result.stdout.strip().split('\n')
            if authors and authors[0]:
                # Return most recent author
                return authors[0]
        except:
            pass
        return "unknown"
    
    async def _calculate_complexity(self, file_path: Path, language: str) -> float:
        """Calculate cyclomatic complexity (simplified version)"""
        if language not in ['python', 'javascript', 'typescript', 'java', 'cpp', 'c']:
            return 0.0
            
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # Simple complexity calculation based on control flow statements
            complexity_keywords = [
                'if', 'elif', 'else', 'for', 'while', 'try', 'except', 'catch',
                'switch', 'case', 'default', 'break', 'continue', 'return'
            ]
            
            complexity = 1  # Base complexity
            for keyword in complexity_keywords:
                complexity += content.count(f' {keyword} ') + content.count(f'\t{keyword} ')
                
            return min(complexity / max(1, content.count('\n')), 10.0)  # Normalize
            
        except:
            return 0.0

    async def _extract_api_signatures(self):
        """Extract public API signatures from code"""
        logger.info("Extracting API signatures")
        # Implementation would use AST parsing for each language
        # For now, store placeholder
        pass
        
    async def _collect_configuration(self):
        """Collect all configuration files and settings"""
        logger.info("Collecting configuration files")
        
        config_files = []
        config_patterns = [
            "*.yml", "*.yaml", "*.json", "*.toml", "*.ini", "*.cfg",
            "Dockerfile*", "docker-compose*", ".env*", "Makefile",
            "*.conf", "*.config", "requirements*.txt", "package.json",
            "pyproject.toml", "setup.py", "setup.cfg"
        ]
        
        for pattern in config_patterns:
            config_files.extend(self.workspace_path.rglob(pattern))
            
        # Save configuration inventory
        config_inventory = []
        for config_file in config_files:
            if not self._should_exclude_file(config_file):
                config_inventory.append({
                    "path": str(config_file.relative_to(self.workspace_path)),
                    "type": self._detect_config_type(config_file),
                    "size": config_file.stat().st_size
                })
                
        with open(self.output_dir / "configuration_inventory.json", 'w') as f:
            json.dump(config_inventory, f, indent=2)
            
    def _detect_config_type(self, file_path: Path) -> str:
        """Detect type of configuration file"""
        name = file_path.name.lower()
        if 'docker' in name:
            return 'container'
        elif any(x in name for x in ['env', 'environment']):
            return 'environment'
        elif any(x in name for x in ['requirements', 'package', 'pyproject', 'setup']):
            return 'dependency'
        elif name == 'makefile':
            return 'build'
        else:
            return 'general'

    async def _analyze_tests(self):
        """Analyze test structure and coverage"""
        logger.info("Analyzing test structure")
        
        test_files = []
        test_dirs = ['tests', 'test', '__tests__', 'spec']
        
        for test_dir in test_dirs:
            test_path = self.workspace_path / test_dir
            if test_path.exists():
                test_files.extend(test_path.rglob("test_*.py"))
                test_files.extend(test_path.rglob("*_test.py"))
                test_files.extend(test_path.rglob("*.test.js"))
                test_files.extend(test_path.rglob("*.spec.js"))
                
        test_inventory = []
        for test_file in test_files:
            if test_file.is_file():
                test_inventory.append({
                    "path": str(test_file.relative_to(self.workspace_path)),
                    "type": self._detect_test_type(test_file),
                    "size": test_file.stat().st_size
                })
                
        with open(self.output_dir / "test_inventory.json", 'w') as f:
            json.dump(test_inventory, f, indent=2)
            
    def _detect_test_type(self, file_path: Path) -> str:
        """Detect type of test file"""
        path_str = str(file_path).lower()
        if 'unit' in path_str:
            return 'unit'
        elif 'integration' in path_str:
            return 'integration'
        elif 'e2e' in path_str or 'end_to_end' in path_str:
            return 'e2e'
        elif 'performance' in path_str:
            return 'performance'
        else:
            return 'unknown'

    async def _dynamic_analysis(self):
        """Step 4: Dynamic Insights - Runtime analysis and profiling"""
        logger.info("Step 4: Performing dynamic analysis")
        
        # Run tests and collect coverage
        await self._run_test_suite()
        
        # Performance profiling (if applicable)
        await self._profile_performance()
        
    async def _run_test_suite(self):
        """Run complete test suite and collect coverage data"""
        logger.info("Running test suite with coverage")
        
        try:
            # Run pytest with coverage
            result = subprocess.run([
                "python", "-m", "pytest", 
                "--cov=src", 
                "--cov-report=json",
                "--cov-report=html",
                "tests/"
            ], capture_output=True, text=True, cwd=self.workspace_path)
            
            # Parse coverage data
            coverage_file = self.workspace_path / "coverage.json"
            if coverage_file.exists():
                with open(coverage_file, 'r') as f:
                    coverage_data = json.load(f)
                    self.test_coverage = coverage_data.get('files', {})
                    
        except Exception as e:
            logger.warning(f"Test execution failed: {e}")
            
    async def _profile_performance(self):
        """Basic performance profiling"""
        logger.info("Profiling performance characteristics")
        
        # Placeholder for performance metrics
        self.performance_metrics = {
            "startup_time": "unknown",
            "memory_usage": "unknown",
            "cpu_usage": "unknown",
            "hotspots": []
        }

    async def _package_metadata(self):
        """Step 5: Metadata & Context Packaging"""
        logger.info("Step 5: Packaging metadata and context")
        
        # Create comprehensive metadata package
        metadata_package = {
            "files": {k: asdict(v) for k, v in self.file_metadata.items()},
            "components": {k: asdict(v) for k, v in self.component_metadata.items()},
            "dependencies": self.dependency_graph,
            "test_coverage": self.test_coverage,
            "performance": self.performance_metrics,
            "scope_config": asdict(self.scope),
            "analysis_timestamp": datetime.now(timezone.utc).isoformat()
        }
        
        # Save metadata package
        with open(self.output_dir / "metadata_package.json", 'w') as f:
            json.dump(metadata_package, f, indent=2)
            
        # Generate architecture overview
        await self._generate_architecture_overview()
        
    async def _generate_architecture_overview(self):
        """Generate high-level architecture overview"""
        overview = {
            "project_name": "NeuroCognitive Architecture",
            "total_files": len(self.file_metadata),
            "languages": list(set(m.language for m in self.file_metadata.values())),
            "total_loc": sum(m.lines_of_code for m in self.file_metadata.values()),
            "complexity_distribution": self._calculate_complexity_distribution(),
            "test_coverage_summary": self._calculate_coverage_summary(),
            "key_components": self._identify_key_components()
        }
        
        with open(self.output_dir / "architecture_overview.json", 'w') as f:
            json.dump(overview, f, indent=2)
            
    def _calculate_complexity_distribution(self) -> Dict[str, int]:
        """Calculate distribution of complexity scores"""
        distribution = {"low": 0, "medium": 0, "high": 0}
        for metadata in self.file_metadata.values():
            if metadata.complexity_score <= 2.0:
                distribution["low"] += 1
            elif metadata.complexity_score <= 5.0:
                distribution["medium"] += 1
            else:
                distribution["high"] += 1
        return distribution
        
    def _calculate_coverage_summary(self) -> Dict[str, float]:
        """Calculate test coverage summary"""
        if not self.test_coverage:
            return {"overall": 0.0, "files_covered": 0}
            
        coverages = [data.get('summary', {}).get('percent_covered', 0) 
                    for data in self.test_coverage.values()]
        
        return {
            "overall": sum(coverages) / len(coverages) if coverages else 0.0,
            "files_covered": len([c for c in coverages if c > 0]),
            "total_files": len(coverages)
        }
        
    def _identify_key_components(self) -> List[Dict[str, Any]]:
        """Identify key architectural components"""
        # Simple heuristic based on file size and complexity
        key_files = []
        for path, metadata in self.file_metadata.items():
            if (metadata.lines_of_code > 100 or metadata.complexity_score > 3.0) and \
               metadata.language in ['python', 'javascript', 'typescript']:
                key_files.append({
                    "path": path,
                    "loc": metadata.lines_of_code,
                    "complexity": metadata.complexity_score,
                    "language": metadata.language
                })
                
        return sorted(key_files, key=lambda x: x['loc'], reverse=True)[:20]

    async def _chunk_and_summarize(self) -> List[Dict[str, Any]]:
        """Step 6: Chunking & Summarization with token awareness"""
        logger.info("Step 6: Chunking and summarizing content")
        
        chunks = []
        context_limit = self.scope.target_llm_profile["context_window"]
        
        # Create hierarchical summaries
        await self._create_hierarchical_summaries()
        
        # Chunk files by token limits
        current_chunk = {"files": [], "token_count": 0, "id": 0}
        
        for path, metadata in self.file_metadata.items():
            # Estimate tokens (rough approximation: 1 token â‰ˆ 4 characters)
            estimated_tokens = metadata.size_bytes // 4
            
            if current_chunk["token_count"] + estimated_tokens > context_limit * 0.8:
                if current_chunk["files"]:
                    chunks.append(current_chunk)
                current_chunk = {"files": [], "token_count": 0, "id": len(chunks)}
                
            current_chunk["files"].append({
                "path": path,
                "metadata": asdict(metadata),
                "estimated_tokens": estimated_tokens
            })
            current_chunk["token_count"] += estimated_tokens
            
        if current_chunk["files"]:
            chunks.append(current_chunk)
            
        # Save chunk manifest
        with open(self.output_dir / "chunk_manifest.json", 'w') as f:
            json.dump(chunks, f, indent=2)
            
        return chunks
        
    async def _create_hierarchical_summaries(self):
        """Create hierarchical summaries at different levels"""
        summaries = {
            "level_0_repo": await self._create_repo_summary(),
            "level_1_packages": await self._create_package_summaries(),
            "level_2_files": await self._create_file_summaries()
        }
        
        with open(self.output_dir / "hierarchical_summaries.json", 'w') as f:
            json.dump(summaries, f, indent=2)
            
    async def _create_repo_summary(self) -> str:
        """Create high-level repository summary"""
        total_files = len(self.file_metadata)
        total_loc = sum(m.lines_of_code for m in self.file_metadata.values())
        languages = set(m.language for m in self.file_metadata.values())
        
        return f"""
        Repository: NeuroCognitive Architecture
        Total Files: {total_files}
        Total Lines of Code: {total_loc}
        Primary Languages: {', '.join(sorted(languages))}
        Architecture: FastAPI-based cognitive agent system with comprehensive testing
        Key Features: LLM integration, automated analysis tools, containerized deployment
        """
        
    async def _create_package_summaries(self) -> Dict[str, str]:
        """Create summaries for each package/module"""
        packages = {}
        
        # Group files by top-level directory
        for path, metadata in self.file_metadata.items():
            parts = Path(path).parts
            if len(parts) > 0:
                package = parts[0]
                if package not in packages:
                    packages[package] = {"files": [], "total_loc": 0}
                packages[package]["files"].append(metadata)
                packages[package]["total_loc"] += metadata.lines_of_code
                
        # Create summaries
        summaries = {}
        for package, data in packages.items():
            summaries[package] = f"""
            Package: {package}
            Files: {len(data['files'])}
            Total LOC: {data['total_loc']}
            Primary Languages: {set(f.language for f in data['files'])}
            """
            
        return summaries
        
    async def _create_file_summaries(self) -> Dict[str, str]:
        """Create summaries for individual files"""
        summaries = {}
        
        for path, metadata in self.file_metadata.items():
            summaries[path] = f"""
            File: {path}
            Language: {metadata.language}
            LOC: {metadata.lines_of_code}
            Complexity: {metadata.complexity_score}
            Last Modified: {metadata.last_modified}
            Author: {metadata.author}
            """
            
        return summaries

    async def _quality_validation(self):
        """Step 7: Quality Gates - Automated validation"""
        logger.info("Step 7: Running quality validation gates")
        
        quality_metrics = {
            "missing_references": await self._check_missing_references(),
            "unresolved_imports": await self._check_unresolved_imports(),
            "token_ratio_outliers": await self._check_token_ratios(),
            "manual_spot_check": await self._perform_spot_check()
        }        # Determine if quality gates pass
        self.quality_gates_passed = all([
            len(quality_metrics["missing_references"]) == 0,
            len(quality_metrics["unresolved_imports"]) == 0,
            len(quality_metrics["token_ratio_outliers"]) < 5
        ])
        
        quality_metrics["overall_passed"] = self.quality_gates_passed
        
        with open(self.output_dir / "quality_metrics.json", 'w') as f:
            json.dump(quality_metrics, f, indent=2)
            
    async def _check_missing_references(self) -> List[str]:
        """Check for missing file references"""
        # Placeholder - would implement reference checking        return []
        
    async def _check_unresolved_imports(self) -> List[str]:
        """Check for unresolved imports"""
        # Placeholder - would implement import resolution checking
        return []
        
    async def _check_token_ratios(self) -> List[str]:
        """Check for unusual token count ratios"""
        # Placeholder - would implement token ratio analysis
        return []
        
    async def _perform_spot_check(self) -> Dict[str, Any]:
        """Perform manual spot check on random 5% of summaries"""
        # Placeholder for manual validation metrics
        return {"files_checked": 0, "issues_found": 0}
        
    async def _prepare_secure_bundle(self, chunks: List[Dict[str, Any]]) -> SummaryBundle:
        """Step 8: Prepare secure transfer bundle"""
        logger.info("Step 8: Preparing secure transfer bundle")
        
        # Load quality metrics from file
        quality_metrics = {}
        quality_metrics_file = self.output_dir / "quality_metrics.json"
        if quality_metrics_file.exists():
            with open(quality_metrics_file, 'r') as f:
                quality_metrics = json.load(f)
        
        # Create summary bundle
        bundle = SummaryBundle(
            metadata={"total_chunks": len(chunks), "quality_passed": self.quality_gates_passed},
            global_overview="Repository overview and architecture summary",
            hierarchical_summaries={},
            chunk_manifest=chunks,
            quality_metrics=quality_metrics,
            checksum="",
            created_at=datetime.now(timezone.utc).isoformat(),
            version="1.0.0"
        )
        
        # Calculate checksum
        bundle_json = json.dumps(asdict(bundle), sort_keys=True)
        bundle.checksum = hashlib.sha256(bundle_json.encode()).hexdigest()
        
        # Save bundle
        bundle_file = self.output_dir / "summary_bundle.json"
        with open(bundle_file, 'w') as f:
            json.dump(asdict(bundle), f, indent=2)
            
        # Create encrypted archive
        await self._create_encrypted_archive(bundle_file)
        
        return bundle
        
    async def _create_encrypted_archive(self, bundle_file: Path):
        """Create encrypted ZIP archive with AES-256"""
        logger.info("Creating encrypted archive")
        
        # Generate encryption key
        password = os.environ.get('SUMMARY_ENCRYPTION_KEY', 'default-key-change-me')
        key = self._derive_key(password.encode())
        fernet = Fernet(key)
        
        # Create ZIP archive
        archive_path = self.output_dir / "summary_bundle_encrypted.zip"
        with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            # Add all analysis artifacts
            for file_path in self.output_dir.rglob("*.json"):
                if file_path != archive_path:
                    zip_file.write(file_path, file_path.relative_to(self.output_dir))
                    
        # Encrypt the archive
        with open(archive_path, 'rb') as f:
            encrypted_data = fernet.encrypt(f.read())
            
        encrypted_path = self.output_dir / "summary_bundle_encrypted.enc"
        with open(encrypted_path, 'wb') as f:
            f.write(encrypted_data)
            
        # Create checksum manifest
        checksum = hashlib.sha256(encrypted_data).hexdigest()
        manifest = {
            "file": "summary_bundle_encrypted.enc",
            "checksum": checksum,
            "created_at": datetime.now(timezone.utc).isoformat(),
            "encryption": "AES-256"
        }
        
        with open(self.output_dir / "transfer_manifest.json", 'w') as f:
            json.dump(manifest, f, indent=2)
            
    def _derive_key(self, password: bytes) -> bytes:
        """Derive encryption key from password"""
        salt = b'neuroca_summary_salt'  # In production, use random salt
        kdf = PBKDF2HMAC(
            algorithm=hashes.SHA256(),
            length=32,
            salt=salt,
            iterations=100000,
        )
        key = base64.urlsafe_b64encode(kdf.derive(password))
        return key

    async def _prepare_validation_suite(self):
        """Step 9: Prepare post-transfer validation"""
        logger.info("Step 9: Preparing validation suite")
        
        validation_suite = {
            "sample_tasks": [
                "Describe the main architecture of this codebase",
                "What testing frameworks are used?",
                "List the main dependencies",
                "Explain the core functionality"
            ],
            "ground_truth_answers": {
                "architecture": "FastAPI-based cognitive agent system",
                "testing": "pytest with coverage requirements",
                "dependencies": "Poetry for Python, FastAPI, pytest",
                "functionality": "LLM integration and cognitive processing"
            },
            "evaluation_metrics": [
                "response_accuracy",
                "hallucination_rate", 
                "response_latency",
                "token_usage"
            ]
        }
        
        with open(self.output_dir / "validation_suite.json", 'w') as f:
            json.dump(validation_suite, f, indent=2)

    async def _setup_automation(self):
        """Step 10: Setup automation and scheduling"""
        logger.info("Step 10: Setting up automation")
        
        # Create automation configuration
        automation_config = {
            "triggers": [
                "on_new_tag",
                "on_merge_to_main", 
                "scheduled_weekly"
            ],
            "pipeline_steps": [
                "environment_prep",
                "static_analysis", 
                "dynamic_analysis",
                "summarization",
                "quality_gates",
                "secure_transfer"
            ],
            "alerting": {
                "on_failure": ["email", "slack"],
                "on_completion": ["email"],
                "on_drift_threshold": ["slack"]
            }
        }
        
        with open(self.output_dir / "automation_config.json", 'w') as f:
            json.dump(automation_config, f, indent=2)

    async def _generate_documentation(self):
        """Step 11: Generate documentation and handoff materials"""
        logger.info("Step 11: Generating documentation")
        
        # Create comprehensive README
        readme_content = """
# Automated Codebase Summarization System

## Overview
This system implements a comprehensive 11-step process for automated codebase analysis,
summarization, and secure transfer to target LLM systems.

## Pipeline Steps
1. **Scoping & Guardrails** - Define analysis scope and security constraints
2. **Environment Prep** - Isolated analysis environment with build toolchain
3. **Static Harvest** - File enumeration, API extraction, configuration collection
4. **Dynamic Insights** - Test execution, performance profiling, runtime metrics
5. **Metadata Packaging** - Comprehensive metadata and context organization
6. **Chunking & Summarization** - Token-aware content chunking and hierarchical summaries
7. **Quality Gates** - Automated validation and integrity checking
8. **Secure Transfer** - Encrypted bundle preparation with checksums
9. **Post-Transfer Validation** - Validation suite and ground truth comparison
10. **Automation & Scheduling** - CI/CD integration and monitoring
11. **Documentation & Handoff** - Complete documentation and maintenance guides

## Usage
```python
from neuroca.analysis.summarization_engine import CodebaseSummarizationEngine, ScopeConfig

# Configure analysis scope
scope = ScopeConfig(
    repos=["main-repo"],
    branches=["main"],
    target_llm_profile={
        "context_window": 128000,
        "input_limits": 100000
    }
)

# Execute pipeline
engine = CodebaseSummarizationEngine(scope, "/path/to/workspace")
bundle = await engine.execute_full_pipeline()
```

## Output Artifacts
- `metadata_package.json` - Comprehensive file and component metadata
- `hierarchical_summaries.json` - Multi-level summaries
- `chunk_manifest.json` - Token-aware content chunks
- `quality_metrics.json` - Validation results
- `summary_bundle_encrypted.enc` - Secure transfer bundle
- `validation_suite.json` - Post-transfer validation materials

## Maintenance
- Update scope configuration for new repositories
- Review quality gates and thresholds quarterly
- Monitor automation pipeline health
- Validate LLM transfer success rates

## Security
- All transfer bundles are AES-256 encrypted
- Checksums verify integrity
- PII and secrets are filtered during analysis
- Access controls on analysis artifacts
"""
        
        with open(self.output_dir / "README.md", 'w') as f:
            f.write(readme_content)
            
        # Create ownership matrix
        ownership_matrix = {
            "system_owner": "NeuroCognitive Architecture Team",
            "technical_lead": "Development Team",
            "security_reviewer": "Security Team", 
            "automation_maintainer": "DevOps Team",
            "documentation_maintainer": "Technical Writing Team",
            "last_updated": datetime.now(timezone.utc).isoformat()
        }
        
        with open(self.output_dir / "ownership_matrix.json", 'w') as f:
            json.dump(ownership_matrix, f, indent=2)
            
        logger.info("Documentation generation complete")


# Factory function for easy instantiation
async def create_summarization_engine(
    workspace_path: str,
    repos: List[str] = None,
    context_window: int = 128000
) -> CodebaseSummarizationEngine:
    """Factory function to create a configured summarization engine"""
    
    if repos is None:
        repos = ["main"]
        
    scope = ScopeConfig(
        repos=repos,
        branches=["main"],
        target_llm_profile={
            "context_window": context_window,
            "input_limits": context_window * 0.8,
            "accepted_formats": ["json", "markdown", "yaml"],
            "rate_limits": {"requests_per_minute": 60}
        }
    )
    
    return CodebaseSummarizationEngine(scope, workspace_path)
